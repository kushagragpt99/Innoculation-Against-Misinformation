---
title: "First Draft Project: Data Analysis and Experiment Planning Tutorial"
output:
  html_document:
    highlight: haddock
    number_sections: no
    theme: journal
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
date: "April 2022"
---

```{r setup, echo = F}
knitr::opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

# Introduction

In this tutorial, we will walk through how to analyze the pilot data for the First Draft project that has been collected through Qualtrics. We will also discuss how to plan the experiment based on the pilot data. 

Let's first load the relevant packages: 

```{r load_packages}
library(tidyverse) # for working with data
library(knitr) # for kable tables
library(here) # for accessing paths to files
library(kableExtra) # for pretty kable tables
library(broom) # for tidy statistical test output
library(lmtest) # for robust standard error calculation
library(sandwich) # for robust standard error calculation
library(moments) # for calculating skewness of distributions
library(zipcodeR)
```


### Read and Clean Data

```{r}
# read in the data
data <- read.csv(here("data", "FDPilot.csv"))  
data <- data[which(data$Finished == "True"),] # remove incomplete observations
```

Questions:
Do we need to worry about the attention check? (`Attention`)

As noted in the First Draft Project Packet, for the pilot, we included as treatments four courses on misinformation, Tactics, Tactics with Quiz Question, Combined (Tactics + Emotion), and Combined with Quiz. Respondents are randomly assigned into one of the five arms (i.e., control and each of the four treatments).

We first need to sort out each respondent's treatment status. For the pilot's survey script, respondents who are assigned to a treatment will be forced to select "Got it." at the end of the treatment messaging. For our convenience, we will transform these variables into boolean (`TRUE` vs. `FALSE`). 

```{r}
# The variable for treatment assignment of each participant is 
# currently either "Got it." or  "". Let's transform them into 
# boolean (true or false):

data$Control <- data$Control == "Got it."

# Here, (data$Control == "Got it.") evaluates whether this is
# true, and assigns the boolean truth value back to the binary
# variable, so Control will be TRUE if previously Control == 
# "Got it." and FALSE otherwise. We can do the same to the other
# binary variables for treatment status:

data$Tactics <- data$Tactics == "Got it."
data$TacticQuiz <- data$TacticQuiz == "Got it."
data$Combined <- data$Combined == "Got it."
data$CombinedQuiz <- data$CombinedQuiz == "Got it."
```

We can also create a new categorical variable called `treatment` that specifies each respondent's treatment status:

```{r}
# Let's make one treatment variable that shows the exact 
# treatment each participant received. Because of the multiple
# cases involved, we use the case_when() function in the dplyr
# package:
 
data <- data %>%
  mutate(treatment = case_when(
      Control ~ 'Control', 
      Tactics  ~ 'Tactics', 
      TacticQuiz ~ 'TacticQuiz', 
      Combined ~ 'Combined', 
      CombinedQuiz ~ 'CombinedQuiz'))

# To facilitate future analysis, let's make sure that
# the baseline of treatment levels is "Control":

data$treatment <- relevel(as.factor(data$treatment), 
                          ref = "Control")

# Let's take a look at the distribution of the treatment
# assignments:

table(data$treatment)
```


Moreover, because we asked many questions on respondent characteristics, we want to see whether each respondent is still concentrating on answering the questions before the treatment is given. Therefore, we had an *attention check question* that is very easy to answer correctly if the respondent is paying attention at all. In the pilot, the attention check asks the respondent to select `"Please go on answering the questions."`. Let's convert the `Attention` variable into a boolean and keep data from participants who passed the attention check.

```{r}
# We first convert Attention into a boolean as above:

data$Attention <- data$Attention == "Please go on answering the questions."

# Let's quickly see what percentage of the respondents passed 
# the attention check. We can do a simple mean calculation
# because in R (as in many other languages), TRUE is numerically 
# equivalent to 1 and FALSE equivalent to 0.

mean(data$Attention)

# Let's only use the data from the participants who have passed 
# the attention checks. There are multiple ways to do this. For
# instance, we can use the filter() function in dplyr: 

data <- data %>%
  filter(Attention == TRUE)
```


# Covariate Balance

As noted in Tutorial 1, before analyzing experiment data, we need to assess covariate balance, in order to make sure that we have indeed randomly assigned the respondents into the control and treatment arms. (We do NOT want to see, for example, that liberals are significantly and substantially more likely to be assigned into one arm, and conservatives another.) Again, as an example, we will evaluate balance on `Ideology`. Let's quickly preview the data type and distribution. 

```{r}
table(data$Ideology)
```

We will convert this to a continuous variable `Ideology_numeric`, where -2 represents very liberal, -1 liberal, 0 moderate, 1 conservative, and 2 very conservative: 

```{r}
# Again, let's use the case_when() function in the dplyr
# package because of the multiple cases here:

data <- data %>%
  mutate(Ideology_numeric = case_when(
      Ideology == "Very liberal" ~ -2, 
      Ideology == "Moderately liberal" ~ -1,
      Ideology == "Moderate" ~ 0,
      Ideology == "Moderately conservative" ~ 1,
      Ideology == "Very conservative" ~ 2
))

table(data$Ideology_numeric)
```

Then we can check the balance on ideology using the following code. 

```{r}
# Generate summary statistics:
covariate_balance_ideology <- data %>%
  select(treatment, Ideology_numeric) %>%
  group_by(treatment) %>%
  summarize(mean = mean(Ideology_numeric), 
            se = sd(Ideology_numeric)/sqrt(n()))

# Generate HTML-friendly table:
kable(covariate_balance_ideology, 
      caption = "Covariate Balance on Ideology in Each Arm",
      # format numbers
      digits = 3,
      format.args = list(big.mark = ",", 
                         scientific = F)) %>%
# Format table:
kable_styling(bootstrap_options = "striped")
```

As we can see, none of the arms seem to have significantly more liberal or more conservative respondents. There are several ways we can confirm this. One way is to run a t-test between each treatment arm and the control arm.

```{r}
# We first get all combinations of arms (for later use):
arm_combos <- combn(unique(data$treatment), 2, simplify = F)

# The first 4 combos are between each treatment arm and control:
arm_combos_control <- arm_combos[1:4]

# We generate an empty data frame that we will add rows to:
t_table_ideology_balance <- data.frame()

# Iterate over each pair of treatment vs. control:
for(i in 1:length(arm_combos_control)){
  arm <- arm_combos_control[[i]]
  # compute pairwise t-test between arms
  t_pairwise <- t.test(Ideology_numeric ~ treatment, 
                       data = data[data$treatment %in% arm, ])%>% 
    # table to data fame
    tidy() %>%
    # rename variables for readability
    rename(difference = estimate,
           tstat = statistic) %>%
    # compute standard error
    mutate(se = difference/tstat, 
           # label which comparison was run
           comparison = paste(arm[[1]], "-", arm[[2]])) %>%
    # select values of interest
    select(comparison, difference, se, p.value)
  # append to our running t-test table
  t_table_ideology_balance <- rbind(t_table_ideology_balance, t_pairwise)
}

# Display t-table:
t_table_ideology_balance %>%
  # generate HTML-friendly table:
  kable(caption = "Covariate Balance on Ideology between Control and Treatments",
        # format numbers
        digits = 3,
        format.args = list(big.mark = ",", 
                           scientific = F)) %>%
  # format table
  kable_styling(bootstrap_options = "striped")
```

Alternatively, we can run a linear regression between `Ideology_numeric` and `treatment`:   

```{r}
# Regress Ideology_numeric over treatment:
ols_ideology_balance <- lm(Ideology_numeric ~ treatment, data)
# Calculate robust standard errors:
coeftest(ols_ideology_balance, vcov = vcovHC(ols_ideology_balance, type="HC1"))
```

As shown above, the two methods lead to similar results: None of the treatment arms have significantly more liberal/conservative respondents than the control arm. Note that in general we do NOT want any significant differences in pre-treatment covariates across the arms. If the assignment of treatment is properly randomized, there should NOT be any significant difference in any pre-treatment covariate among different arms. 



# Summary Statistics

We now proceed to calculate an visualize the summary statistics for post-treatment outcomes, namely, the sample mean, sample standard deviation, and estimated standard error for each arm. 

In the pilot, we have included multiple outcome measures. (Please refer to the Project Packet and Pilot Qualtrics Script for details.) As an example of data analysis, here our outcome of interest is confidence-weighted measures of accuracy in identifying misinformation. After treatment, each respondent sees ten social media posts, six of which contain misinformation (three focused on tactics, three on emotions). We ask the respondents whether they think each post contains misinformation ("Yes", "No", and "I can't tell", recorded as `X01TF`, `X02TF`, ..., `X10TF`) and how confident they are in their judgment (on a scale of 0~10, recorded as `X01C_1`, `X02C_1`, ..., `X10C_1`). 

In general, it is always a good habit to look at the data before performing the analysis.

```{r}
# We want to see what kinds of values the outcome variable has:
table(data$X01TF)
```

We can then create score variables for the posts:

```{r}
# Posts 1, 4, 5, 7, 8, and 9 contain misinformation: 

data <- data %>%
  mutate(A1 = case_when(
      X01TF == "Yes, I think it contains misinformation" ~ 1, 
      X01TF == "No, I don't think it contains misinformation" ~ -1,
      X01TF == "I can't tell" ~ 0))

data <- data %>%
  mutate(A4 = case_when(
      X04TF == "Yes, I think it contains misinformation" ~ 1, 
      X04TF == "No, I don't think it contains misinformation" ~ -1,
      X04TF == "I can't tell" ~ 0))

data <- data %>%
  mutate(A5 = case_when(
      X05TF == "Yes, I think it contains misinformation" ~ 1, 
      X05TF == "No, I don't think it contains misinformation" ~ -1,
      X05TF == "I can't tell" ~ 0))

data <- data %>%
  mutate(A7 = case_when(
      X07TF == "Yes, I think it contains misinformation" ~ 1, 
      X07TF == "No, I don't think it contains misinformation" ~ -1,
      X07TF == "I can't tell" ~ 0))

data <- data %>%
  mutate(A8 = case_when(
      X08TF == "Yes, I think it contains misinformation" ~ 1, 
      X08TF == "No, I don't think it contains misinformation" ~ -1,
      X08TF == "I can't tell" ~ 0))

data <- data %>%
  mutate(A9 = case_when(
      X09TF == "Yes, I think it contains misinformation" ~ 1, 
      X09TF == "No, I don't think it contains misinformation" ~ -1,
      X09TF == "I can't tell" ~ 0))


# Posts 2, 3, 6, and 10 do NOT contain misinformation: 

data <- data %>%
  mutate(A2 = case_when(
      X02TF == "Yes, I think it contains misinformation" ~ -1, 
      X02TF == "No, I don't think it contains misinformation" ~ 1,
      X02TF == "I can't tell" ~ 0))

data <- data %>%
  mutate(A3 = case_when(
      X03TF == "Yes, I think it contains misinformation" ~ -1, 
      X03TF == "No, I don't think it contains misinformation" ~ 1,
      X03TF == "I can't tell" ~ 0))

data <- data %>%
  mutate(A6 = case_when(
      X06TF == "Yes, I think it contains misinformation" ~ -1, 
      X06TF == "No, I don't think it contains misinformation" ~ 1,
      X06TF == "I can't tell" ~ 0))

data <- data %>%
  mutate(A10 = case_when(
      X10TF == "Yes, I think it contains misinformation" ~ -1, 
      X10TF == "No, I don't think it contains misinformation" ~ 1,
      X10TF == "I can't tell" ~ 0))

```

One way to calculate the weighted misinformation ID accuracy is to assign a score of 1 for each post correctly identified, -1 for each post incorrectly identified (0 for saying "I can't tell"), and multiple the score by their confidence level. There can be other ways to implement the scoring, such as normalizing each respondent's self-reported confidence levels. But for the sake of this tutorial, we will demonstrate the most straightforward way. 

```{r}
data$Accuracy_conf <- (data$A1 * as.numeric(data$X01C_1) + data$A2 * as.numeric(data$X02C_1) + 
                      data$A3 * as.numeric(data$X03C_1) + data$A4 * as.numeric(data$X04C_1) + 
                      data$A5 * as.numeric(data$X05C_1) + data$A6 * as.numeric(data$X06C_1) + 
                      data$A7 * as.numeric(data$X07C_1) + data$A8 * as.numeric(data$X08C_1) + 
                      data$A9 * as.numeric(data$X09C_1) + data$A10 *as.numeric(data$X10C_1))/10

summary(data$Accuracy_conf)
```

Now we can use similar code to calculate and display the summary statistics for `Accuracy_conf` for each treatment and control arm. 

```{r}
means_accuracy_conf <- data %>%
  select(treatment, Accuracy_conf) %>%
  group_by(treatment) %>%
  summarize(mean = mean(Accuracy_conf), 
            sd = sd(Accuracy_conf), 
            n=n(), 
            se = sd/sqrt(n))

kable(means_accuracy_conf, 
      caption = "Mean by Arm",
  digits = 3,
  format.args = list(big.mark = ",", 
                     scientific = F)) %>%
  kable_styling(bootstrap_options = "striped") 
```

Below is the code for visualizing the above data with the *ggplot* package. 

You do not need to understand how to use the package in detail. But you can find some guide on R data visualization online, such as http://r-statistics.co/ggplot2-Tutorial-With-R.html. If you have questions, please consult the teaching team.  

```{r}
# Create graph where x-axis is the treatment arm, y-axis 
# is the outcome, and color by treatment arm. 
ggplot(means_accuracy_conf, 
       aes(x = treatment, y = mean, 
           group = treatment, color = treatment)) +
  # add bars for 95% confidence interval
  geom_errorbar(aes(ymin = mean - 1.96*se, ymax = mean + 1.96*se),
              width = .1,
              position=position_dodge(width=0.5)) + 
  # add points for average outcome
  geom_point(size =3, position=position_dodge(width=0.5)) +
  # add function to make the treatment labels readable
  scale_x_discrete(labels = function(x) str_replace(str_replace_all(x, "\\_", " "), " ", "\\\n")) + 
    # adjust y-axis scale so we can see the means easily
  scale_y_continuous(breaks = seq(1, 5, 1), 
                     labels = seq(1, 5, 1), 
                     limits = c(1, 4.5)) + 
  # add labels
  labs(title = "Confidence-weighted Accuracy Score \n in Misinformation Identification", 
       x = 'Treatment', y = "Accuracy Score", 
       caption  = "For participants who passed the attention check (N = 291); 95% CI displayed.") +
  # adjust theme for aesthetics
  theme_minimal() + 
  # change text size and face
  theme(strip.text = element_text(size = 12), 
        axis.text = element_text(size = 7), 
        axis.title = element_text(size = 12, face = "bold"), 
        legend.position = "none", 
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5))
```


# Comparing Outcomes Across Different Treatment Arms

One important objective for analyzing experiment data is to evaluate whether there is a significant difference in the outcome (`Accuracy_conf`) between the control arm and the treatment arms. A simple evaluation is to conduct pair-wise t-tests. We can use similar code as above for this analysis. 

```{r pairwise_t_tests}
# We will conduct pairwise t-tests. Note that we have already
# created the variable arm_combos above, which includes all
# pairwise combinations of arms when doing the covariate 
# balance checks. We can also recycle the code for that task. 

t_table <- data.frame()

for(i in 1:length(arm_combos)){
  arm <- arm_combos[[i]]
  t_pairwise <- t.test(Accuracy_conf ~ treatment, 
                       data = data[data$treatment %in% arm, ])%>% 
    tidy() %>%
    rename(difference = estimate,
           tstat = statistic) %>%
    mutate(se = difference/tstat, 
           comparison = paste(arm[[1]], "-", arm[[2]])) %>%
    select(comparison, difference, se, p.value)
  t_table <- rbind(t_table, t_pairwise)
}

t_table %>%
  kable(caption = "Confidence-weighted Accuracy Score \n in Misinformation Identification: \n 
        Difference in Between Means by Arm",
        digits = 3,
        format.args = list(big.mark = ",", 
                           scientific = F)) %>%
  kable_styling(bootstrap_options = "striped")
```

Taken alone (e.g., not accounting for multiple hypothesis test considerations), the table above shows that the `Tactics` treatment arm has significantly (at the 95% level) higher outcomes than `Control`, suggesting that the treatment is effective in enhancing respondent ability to identify misinformation confidently. However, the sample is in general too small to draw strong conclusions, as evidenced by the large confidence intervals.

Another way to quickly assess the magnitude and significance of treatment effects is to run a linear regression, where the coefficient on treatment can be interpreted as the treatment effect (because treatment is randomly assigned). A linear regression can be useful if we want to adjust for covariates that may not (by chance) be balanced between treatment and control arms. If one of the arms has more people of a given type, and that type also has systematically different outcomes, it becomes a "confounder" if we don't adjust for it. Here, we adjust for `Ideology_numeric`. Note that here, because `Ideology_numeric` is balanced across arms given proper randomization of treatment assignment, including it in the regression does not make much of a difference.


```{r}
# Regress Accuracy_conf over treatment while 
# controlling for Ideology_numeric:
model <- lm(Accuracy_conf ~ treatment + Ideology_numeric, data)
results <- coeftest(model, vcov = vcovHC(model, type="HC1"))
results
```

However, even if none of the other treatment arms have significantly different outcomes from the control arm, it does not mean that the treatments don't work, because we may be underpowered here -- note that each arm has only fewer than 60 respondents. It is possible that with more respondents we may discover a significant effect. 


# Multiple Hypothesis Testing

Given that we have many treatment arms, multiple hypothesis testing becomes a relevant problem: As explained in class, naively looking at whether each arm/outcome has a corresponding p-value below 0.05 would lead to inflated false positives. Here, we demonstrate three frequently used ways to account for multiple hypothesis testing, i.e., the Bonferroni correction, the Holm correction, and the Benjamini-Hochberg correction (please refer to the multiple hypothesis testing tutorial for details). A detailed guide on multiple hypothesis testing is available at https://egap.org/resource/10-things-to-know-about-multiple-comparisons/. 


```{r}
# We first obtain the p-values from the regression results.
# Note that here, the fourth column of the results are 
# p-values, and that the 2~5 rows correspond to the treatment
# arms (see the last code chunk): 

p_values <- results[2:5,4] 

# Because of the limited sample size in the pilot, we use 
# a relatively lenient false positive rate of 0.1 for illustration.
# Conventionally, we often use alpha = 0.05 (for significance
# at the 95% level. 

alpha <- 0.1
p_values < alpha

# We use the p.adjust() function for multiple hypothesis corrections:

p.adjust(p_values, "bonferroni") < alpha

p.adjust(p_values, "holm") < alpha

p.adjust(p_values, "BH") <alpha
```

As the results show, even at the relatively lenient $\alpha = 0.1$ threshold, after multiple hypothesis testing, none of the treatment arms have a significant effect. Again, this may mean that we are underpowered, and it is possible that with more respondents we may discover a significant effect. 

# Covariate Selection

The pilot data contains many covariate questions that are asked before treatment assignment. These covariates include demographic information (age, gender, education, income, etc.), partisanship and ideology, trust levels, and information sources. This section will introduce how you can choose which covariates to include in your actual experiment. We will use trust level of Dr. Fauci (`FauciTrust`) as an example to illustrate this process. 

## Hetreogeneous Treatment Effects

First, as laid out in the project packet, an important goal to include a covariate is to see how it helps us gain understanding about potentially heterogeneous treatment effects among different groups of respondents (i.e., respondents who have different values for the covariate). For instance, we need to collect information on the respondents' trust levels for Dr. Fauci if we believe that those who trust Fauci less will react differently to our treatment than those who trust him more. 

In particular, the social media posts we present to the respondents after treatment are all about medical/health topics, and those who do not trust Fauci are likely to be those who are more susceptible to medical/health-related misinformation than those who do trust Fauci. In other words, our treatment may be more useful to those who trust Fauci less, who have more "room for improvement" in identifying such misinformation. Therefore, one reasonable hypothesis is that the treatments may be more effective in enhancing the ability to identify misinformation among those who trust Fauci less. 

Below, we explore how treatment effects vary based on how much a respondent trusts Fauci.

```{r}
# We first transform FauciTrust into numeric and binary forms.
data <- data%>%
  mutate(FauciTrust_numeric = parse_number(FauciTrust))

data$FauciTrust_binary <- data$FauciTrust_numeric >= 4

# Again, we recycle some of the previously used code for 
# calculating the summary statistics:
means_accuracy_conf_Fauci <- data %>%
  # select our variables of interest
  select(treatment, FauciTrust_binary, Accuracy_conf) %>%
  # group by treatment and ideology
  group_by(treatment, FauciTrust_binary) %>%
  summarize(mean = mean(Accuracy_conf), 
            sd = sd(Accuracy_conf), 
            n=n(), 
            se = sd/sqrt(n))

kable(means_accuracy_conf_Fauci, 
      caption = "Mean by Arm and Fauci Trust Level",
  digits = 3,
  format.args = list(big.mark = ",", 
                     scientific = F)) %>%
  kable_styling(bootstrap_options = "striped") 
```
Let's visualize the results: 

```{r}
means_accuracy_conf_Fauci %>%
  ggplot(aes(treatment, mean, group = FauciTrust_binary, color = treatment)) +
  geom_errorbar(aes(ymin = mean - 1.96*se, ymax = mean + 1.96*se, 
                    linetype = as.factor(FauciTrust_binary)),
              width = .1,
              position=position_dodge(width = .4)) + 
  geom_point(size =3,
             position=position_dodge2(width= .4))  +
  scale_y_continuous(breaks = seq(0, 6 , 1), 
                     labels = seq(0, 6, 1), 
                     limits = c(0, 6)) + 
  scale_x_discrete(labels = function(x)str_replace(str_replace_all(x, "\\_", " "), " ", "\\\n")) + 
  labs(title = "Means and 95% CI by Arm \n and Fauci Trust Level", 
       x = 'Treatment', 
       y = "Accuracy Score", 
       caption  = "For participants who passed the attention check (N = 291); 95% CI displayed. \n Dotted CIs represent estimates for participants who trusts Fauci more (>= 4 on a 5-point scale).") +
  theme_minimal() + 
  theme(strip.text = element_text(size = 12), 
        axis.text = element_text(size = 7), 
        axis.title = element_text(size = 12, face = "bold"), 
        legend.position = "none", 
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5))

```

The plot indeed suggests that those who trust Fauci less react more positively to the treatments than those who trust Fauci more. To evaluate our hypothesis quantitatively, we can run the following regression interacting `treatment` with `FauciTrust_numeric`, which will show the differential effect of the treatments with respect to different Fauci trust levels. 

```{r}
model_2 <- lm(Accuracy_conf ~ treatment * FauciTrust_numeric, data)

results_2 <- coeftest(model_2, vcov = vcovHC(model_2, type="HC1"))
results_2
```
The regression results indicate that there is indeed substantial and highly significant heterogeneity in the treatment effects among respondents who trust Fauci differently: The `CombinedQuiz` treatment is much more effective in boosting ability to identify misinformation among those who trust Fauci less (p-value = 0.029). A similar pattern may be valid for the `TacticQuiz` and `Tactics` treatments as well, as suggested by their relatively low p-values (0.074 and 0.129 respectively). Therefore, Fauci trust level is a good covariate to include in the experiment. 

## Heterogenous Policy Assignment

We also want to evaluate if there are potentially heterogeneous policy assignments, meaning that different values of the covariate results in different optimal policy to treat the respondent - this means that we need not only heterogeneity in treatment, but a switch in what is optimal policy in response. 

Based on the plot and the results above, it seems that for those who trust Fauci less, our treatments have a positive effect, but they do not have much effect (and may even backfire) for those who trust Fauci more. Let's now specifically take a look at those who do not trust Fauci at all (i.e., those who select 1) and those who trust Fauci a great deal (i.e., those who select 5). We can see this by running the following regressions on the two groups separately.  

```{r}
model_3 <- lm(Accuracy_conf ~ treatment, data[which(data$FauciTrust_numeric == 1),])
results_3 <- coeftest(model_3, vcov = vcovHC(model_3, type="HC1"))
results_3

model_4 <- lm(Accuracy_conf ~ treatment, data[which(data$FauciTrust_numeric == 5),])
results_4 <- coeftest(model_4, vcov = vcovHC(model_4, type="HC1"))
results_4
```
Therefore, Fauci trust level as a covariate helps inform optimal policy assignment. In this case, if the actual experiment reveals a similar pattern, we should target those who do not trust Fauci at all with the `CombinedQuiz` treatment, whereas for those who trust Fauci a great deal, we should provide no treatment at all. 

## Expected Variance of the Covariate and Correlation Among Covariates

We also need to have some variation in the covariates. For example, there would be no point in asking about Fauci trust level if all (or almost all) respondents trust him a great deal. This wonâ€™t provide us with valuable insights on heterogeneous treatment effects. Below, we can see that we have a well-rounded distribution of different trust levels. Even for the binary `FauciTrust_binary` variable, the distribution is not skewed excessively toward either value. 

```{r}
table(data$FauciTrust_numeric)
table(data$FauciTrust_binary)
```
It is also important to avoid wasting effort, time, or money in collecting multiple covariates that are highly correlated with one another. For example, the pilot has a question on partisanship and another one on ideology. It is unsurprising that partisanship is highly correlated with ideology.

```{r}
# In the survey, we adopt the best practice from the political
# science literature to ask two questions on party ID. The first
# question (PartyID1) asks if the respondent identifies as Dem
# Rep, Independent, or what. If neither Dem nor Rep, the second
# question (PartyID3) asks whether the respondent feels closer to
# Dem or Rep (because some partisans may declare they are inde-
# pendents if not asked this way. We only categorize a respondent
# as independent if the answer to the second question (PartyID3) 
# remains "Independent" or "Neither". 

data$PartyID3 <- ifelse(data$PartyID3 == "Democratic Party", "Democrat", data$PartyID3)
data$PartyID3 <- ifelse(data$PartyID3 == "Republican Party", "Republican", data$PartyID3)
data$PartyID3 <- ifelse(data$PartyID3 == "Neither", "Independent", data$PartyID3)

data$Party <- ifelse(data$PartyID1 != "Democrat" & data$PartyID1 != "Republican", data$PartyID3, data$PartyID1)

table(data$Party, data$Ideology)
```

Hence, partisanship and ideology provide essentially the same information and will thus be duplicative. Therefore, when selecting a small set of covariates to select, it is useful to examine the correlation matrix of the covariates and avoid choosing two covariates that are highly correlated with one another.


# Outcome Selection

In addition to the main outcomes of interest, i.e., the respondents' ability to identify misinformation, the pilot has included another outcome measure that we may also want to learn about, namely, whether they will share misinformation on social media. For each post, after we ask the respondents to judge whether they think it contains misinformation and how confident they are in their judgment, we also ask whether they will share the post on social media (`X01S` ~ `X10S`). We will use this as an example to analyze which variable to select for the actual experiment based on the criteria listed in the Outcome Selection Tutorial. 

First, we will take a look at what these variables look like (i.e., if they are numeric or strings): 

```{r}
table(data$X01S)
```


As shown above, the variables are stored as text strings, so we need to parse/transform them into numeric forms for further analysis. Note that for convenience, we count both those who say "I may share it to criticize its content" and those who say "Definitely not (sharing)" as 1, because sharing the post with critical language means that the respondent does not intend to spread the misinformation in a positive or neutral way. 

```{r}
# Posts 1, 4, 5, 7, 8, and 9 contain misinformation: 

data <- data %>%
  mutate(S1 = case_when(
    X01S == "I may share it to criticize its content" ~ 1,
    X01S == "Definitely not" ~ 1, 
    X01S == "I probably wouldn't" ~ 2,
    X01S == "I might share it but I'm not sure" ~ 3,
    X01S == "Probably" ~ 4, 
    X01S == "Definitely" ~ 5))

data <- data %>%
  mutate(S4 = case_when(
    X04S == "I may share it to criticize its content" ~ 1,
    X04S == "Definitely not" ~ 1, 
    X04S == "I probably wouldn't" ~ 2,
    X04S == "I might share it but I'm not sure" ~ 3,
    X04S == "Probably" ~ 4, 
    X04S == "Definitely" ~ 5))

data <- data %>%
  mutate(S5 = case_when(
    X05S == "I may share it to criticize its content" ~ 1,
    X05S == "Definitely not" ~ 1, 
    X05S == "I probably wouldn't" ~ 2,
    X05S == "I might share it but I'm not sure" ~ 3,
    X05S == "Probably" ~ 4, 
    X05S == "Definitely" ~ 5))

data <- data %>%
  mutate(S7 = case_when(
    X07S == "I may share it to criticize its content" ~ 1,
    X07S == "Definitely not" ~ 1, 
    X07S == "I probably wouldn't" ~ 2,
    X07S == "I might share it but I'm not sure" ~ 3,
    X07S == "Probably" ~ 4, 
    X07S == "Definitely" ~ 5))

data <- data %>%
  mutate(S8 = case_when(
    X08S == "I may share it to criticize its content" ~ 1,
    X08S == "Definitely not" ~ 1, 
    X08S == "I probably wouldn't" ~ 2,
    X08S == "I might share it but I'm not sure" ~ 3,
    X08S == "Probably" ~ 4, 
    X08S == "Definitely" ~ 5))

data <- data %>%
  mutate(S9 = case_when(
    X09S == "I may share it to criticize its content" ~ 1,
    X09S == "Definitely not" ~ 1, 
    X09S == "I probably wouldn't" ~ 2,
    X09S == "I might share it but I'm not sure" ~ 3,
    X09S == "Probably" ~ 4, 
    X09S == "Definitely" ~ 5))
```

Then, let's use the mean of these variables as our outcome, `S_false`. 

```{r}
data <- data %>%
  mutate(S_false = (S1 + S4 + S5 + S7 + S8 + S9)/6)
```

Note that for `S_false`, the higher the numeric value, the more a respondent may share misinformation on social media. In other words, the goal of our treatment is to decrease the outcome. (In contrast, the goal is for the treatments to increase our main outcome variable, i.e., ability to identify misinformation.)

## Baseline Distributions

As outlined in the Outcome Selection Tutorial, baseline distributions can be very important for determining whether we will be able to detect a treatment effect with sufficient power. For the binary example given in the Outcome Selection Tutorial, the mean provides all the information about the distribution. However, since our outcome `S_false` is on a scale of 1 to 5, knowing only the mean is insufficient for pinning down the baseline distribution. 

```{r}
mean(data$S_false[which(data$Control)])
var(data$S_false[which(data$Control)])
``` 
The mean is not too far from 3 (we do NOT want the mean to be on either end of the range), and the variance is neither too high nor too low. (As noted in the Outcome Selection Tutorial, the higher the variance of the outcome variability, the lower the power and the higher the required sample size for detecting a given treatment effect size, so, too high variance of the outcome is problematic, but so is too low variance. In many settings, when the outcome value doesn't vary across individuals, it is also hard to change the outcome very much -- the underlying factors that lead the outcome to be the same for most also make it hard to change those.)

But when we examine the distribution, we find that it is quite right-skewed: 

```{r}
hist(data$S_false[which(data$Control)], 
     main = "Baseline Distribution", 
     xlab = "inclination to sharing misinformation")

skewness(data$S_false[which(data$Control)])
``` 
This could be problematic. Intuitively, when we have more people who have already had the lowest possible outcome value (1) here, their value will probably not change when they receive a treatment that is intended to lower this value. 

## Constructing Control and Treatment Arms for Simulations

As is with the binary example in the Outcome Selection Tutorial, we need to make some reasonable assumption on how people may respond to our treatment. Here, we make the assumption that some proportion $q$ of respondents will have their value reduced by 2/6 (i.e., two decreases of 1 or one decrease of 2 on the six misinformation posts) after treatment if they would have had a value higher than 1 without treatment. (Those who would already have had 1 without treatment cannot be further treated. For simplicity, we assume that those who have had 7/6 cannot be treated either. As calculated below, approximately 74% of the respondents have the outcome value higher than 7/6.) 

```{r}
mean(data$S_false[which(data$Control)] > 7/6)
``` 
This means that the average treatment effect will be $q \times (2/6) \times 74\%  = 0.25 q$. This is a highly simplified assumption because in reality it is possible that some respondent's individual level treatment effect is greater than 2/6. For instance, one would have had 5 for a post without treatment but has 2 after receiving treatment. So why are we making this assumption? In addition to convenience, note that the baseline distribution is right-skewed, so we will typically have lower variance given the same average treatment effect if some respondents have individual level treatment effects greater than 2/6, which will push the distribution further skewed toward the right than the latter. Therefore, the variance of our synthetic treatment arm based on this simplified assumption is likely to be overestimated, meaning that we are unlikely to be underpowered because of this assumption. 

(You may have noticed that here we are also implicitly assuming that the treatment would not backfire, i.e., no respondent who receives the treatment would have selected a lower value without treatment. In our context, this seems like a mild assumption, because there does not appear to be anything in our treatments that could lead a respondent to become **more** inclined to sharing misinformation.)

Then, we can use bootstrapping to simulate what will happen for a sample size $n$ and a treatment effect $0.25q$. To do this, we will randomly select $n$ times from the baseline distribution of `S_false` in our pilot data (with replacement, meaning that if we choose an observation this time, we may still choose it again next time) to construct a control arm with $n$ observations. Then, we randomly choose a proportion $q$ of the observations to have their values reduced by 2/6 (unless their value is already 1 or 7/6) to construct our treatment arm. 

We can then run a t-test between the constructed control and treatment arms and record the p-value. Repeat this process multiple times (e.g., for 1000 trials) and we can estimate the power for a given $n$ and $q$, i.e., the proportion of trials where the p-value falls below some threshold (e.g., $\alpha = 0.05$) so that we can (correctly) reject the null hypothesis that the average treatment effect is 0. Let's see how many observations we will need for each arm when we set $q=0.8$ (i.e., for an average treatment effect of $0.25 q  = 0.2$) if we want to achieve a power of 0.8 for each variable: 

```{r}
set.seed(94305)

q <- 0.8
alpha <- 0.05

trial <- function(n, q, control_dist){
  control_dist_n <- sample(control_dist, n, replace = T)
  treat_dist_n <- pmax(1, control_dist_n - (rbinom(n, size = 1, prob = q)) * (2/6))
  p_value <- t.test(control_dist_n, treat_dist_n)$p.value
  return(p_value)
}

wrapper_replicate <- function(n){
  p_values <- replicate(1000, trial(n, q, control_dist))
  return(mean(p_values < alpha))
}

n_range <- 100:250

control_dist <- data$S_false[which(data$Control)]
power <- sapply(n_range, wrapper_replicate)
d <- data.frame(n_range, power)

ggplot(d, aes(x=n_range, y=power)) +
  geom_point()  + 
  geom_hline(yintercept=0.8, linetype="dashed", color = "grey") +
  # add labels
  labs(x = 'sample size (n)', y = "power", 
       caption  = "q = 0.8, alpha = 0.05") +
  # adjust theme for aesthetics
  theme_minimal()
```

As the plot above shows, we need approximately $n = 200$ for each arm to achieve a power of 0.8. If we only had, say, $n = 150$ for each arm as a budget constraint (don't forget we're testing multiple treatment arms!), we would only be able to achieve a power of 0.125. This discrepancy speaks to the importance of carefully analyzing power when selecting an outcome!  

You can change the value of $q$ and $\alpha$ to see how the power changes. Importantly, when you are testing multiple hypotheses, it is important to address the multiple comparison problem by adjusting the $\alpha$ threshold. (Please refer to the Multiple Hypothesis Testing section for details.) 

## Substantive Considerations

In addition to statistical considerations, we also need to pay attention to substantive issues when selecting outcome measures, especially on how the outcome may relate to the project goals and to our treatments.

Here, the specific project goal of including a measure like `S_false` is to evaluate whether our treatment can mitigate the sharing of misinformation on social media. Recall the "impact funnel" (Information -> Attitude -> Plans -> Action) laid out in the Outcome Selection Tutorial. 

Our treatment is informational (courses on misinformation), and our main outcome is attitudinal, which can directly link to the treatment. Moreover, the measure is objective: We score the respondents based on their judgment rather than ask them to rate their ability to identify misinformation themselves. 

On the other hand, `S_false` is a (self-reported) action outcome, which is a bit more remote on the funnel, but it is still arguably related to our treatment (because the inoculation against misinformation courses have explicitly warned the respondents against sharing misinformation posts on social media). While we cannot measure the actual action of sharing, and self-reported sharing inclinations can vary from actual sharing behavior, we do not expect the difference to be a huge one (because sharing information online is relatively not costly, unlike physical actions such as voting), or concentrated among only the treatment arm or the control, which would bias our results. Therefore, `S_false` appears to be an appropriate, even if imperfect, outcome to include. 

## Pilot Results on the Outcomes

Finally, we can check out the pilot results on the outcomes, e.g., by running a regression of the outcomes on treatment. As noted above, the results are for reference only: Because of the pilot's limited sample size, insignificant results are not particularly informative. However, we can see that the `TacticQuiz` treatment is significant. Moreover, `Combined` and `Tactics` treatments have fairly low corresponding p-values. All three of these treatments have shown substantial effect size as well (approximately -0.34 to -0.4, larger than our conjectured effect of -0.2). This is a promising result.  

```{r}
model_5 <- lm(S_false ~ treatment, data)
results_5 <- coeftest(model_5, vcov = vcovHC(model_5, type="HC1"))
results_5

```
You should go through a similar process to consider the relevant statistical and substantive properties when you select other outcomes to include based on the pilot data. For instance, one may be concerned that the treatments may reduce the sharing of accurate information, or make people incorrectly believe that certain accurate information is false. You can find the relevant pilot data to explore these outcomes. 


```{r}
data
```


```{r}
states <- map_data("state") %>% 
  select(lon = long, lat, group, id = subregion)
```

```{r}
ggplot() +
  geom_polygon(data = states, mapping = aes(lon, lat, group = group), fill = "white", color = "grey50") +
  geom_point(data = data, mapping = aes(x = LocationLongitude, y = LocationLatitude, color = Accuracy_conf)) +
  coord_quickmap(xlim = c(-125, -68), ylim = c(25, 50)) +
  scale_color_gradient2(low = "red", mid = "goldenrod", high = "darkgreen") +
  theme_minimal()
```

```{r}
ggplot() +
  geom_polygon(data = states, mapping = aes(lon, lat, group = group), fill = "white", color = "grey50") +
  geom_point(data = data, mapping = aes(x = LocationLongitude, y = LocationLatitude, color = Accuracy_conf)) +
  coord_quickmap(xlim = c(-125, -68), ylim = c(25, 50)) +
  scale_color_gradient2(low = "red", mid = "goldenrod", high = "darkgreen") +
  theme_minimal() +
  facet_wrap(.~treatment)
```

```{r}
zip_list <- tibble()

for(i in 1:nrow(data)){
  
  print(i)
  
  zips <- search_radius(data$LocationLatitude[i], data$LocationLongitude[i], radius = 10)
  
  if(nrow(zips) > 0){
  closest_zip <- zips %>% 
    filter(distance == min(distance)) %>% 
    mutate(X = data$X[i])
  
  zip_list <- zip_list %>% 
    rbind(closest_zip)
  }

}

zip_list %>% 
  write_csv("data/zip_list.csv")
```

```{r}
data_geo <- data %>% 
  left_join(zip_list) %>% 
  left_join(zip_code_db) %>% 
  mutate(population_density_log = log(population_density))
```

```{r}
data_geo %>% 
  ggplot(aes(population_density)) +
  geom_histogram()
```

```{r}
data_geo %>% 
  ggplot(aes(log(population_density))) +
  geom_histogram()
```

```{r}
data_geo %>% 
  ggplot(aes(population_density, population_density_log)) +
  geom_point()
```

```{r}
model_density <- lm(Accuracy_conf ~ treatment * population_density_log, data_geo)

model_density %>% 
  summary()

results_density <- coeftest(model_density, vcov = vcovHC(model_density, type="HC1"))
results_density
```

```{r}
model_time <- lm(Accuracy_conf ~ treatment * log(Duration..in.seconds.), data_geo)

model_time %>% 
  summary()

results_time <- coeftest(model_time, vcov = vcovHC(model_time, type="HC1"))
results_time
```

```{r}
model_time <- lm(Accuracy_conf ~ treatment * log(Duration..in.seconds.), data_geo %>% filter(Duration..in.seconds. < 13107))

model_time %>% 
  summary()

results_time <- coeftest(model_time, vcov = vcovHC(model_time, type="HC1"))
results_time
```

```{r}
data_geo %>% 
  ggplot(aes(population_density_log, Accuracy_conf, color = treatment)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_classic() +
  facet_wrap(.~treatment)
```

```{r}
data_geo %>% 
  ggplot(aes(log(Duration..in.seconds.), Accuracy_conf, color = treatment)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_classic() +
  facet_wrap(.~treatment)
```

```{r}
data_geo %>% 
  filter(Duration..in.seconds. < 13107) %>% 
  ggplot(aes(log(Duration..in.seconds.), Accuracy_conf, color = treatment)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_classic() +
  facet_wrap(.~treatment)
```


```{r}
data_geo %>% 
  arrange(desc(Duration..in.seconds.))
```

```{r}
model_1 <- lm(Accuracy_conf ~ treatment, data)
```

```{r}
model_1 %>% 
  summary()
```

```{r}
results_1 <- coeftest(model_1, vcov = vcovHC(model_5, type="HC1"))

results_1
```

```{r}
data %>% 
  count(treatment)
```


```{r}
t.test(data %>% filter(treatment == "Combined") %>% pull(Accuracy_conf),
       data %>% filter(treatment == "Control") %>% pull(Accuracy_conf)) %>% broom::tidy()
```

```{r}
t.test(data %>% filter(treatment == "CombinedQuiz") %>% pull(Accuracy_conf),
       data %>% filter(treatment == "Control") %>% pull(Accuracy_conf)) %>% broom::tidy()
```

```{r}
t.test(data %>% filter(treatment == "TacticQuiz") %>% pull(Accuracy_conf),
       data %>% filter(treatment == "Control") %>% pull(Accuracy_conf)) %>% broom::tidy()
```

```{r}
t.test(data %>% filter(treatment == "Tactics") %>% pull(Accuracy_conf),
       data %>% filter(treatment == "Control") %>% pull(Accuracy_conf)) %>% broom::tidy()
```

```{r}
calculate_power <- function(condition = condition,
                            n = n,
                            estimate = NA){
  
  if(is.na(estimate)){
      estimate <- t.test(data %>% filter(treatment == condition) %>% pull(Accuracy_conf),
       data %>% filter(treatment == "Control") %>% pull(Accuracy_conf)) %>% 
    broom::tidy() %>% 
    pull(estimate)
  }

  
  sd <- data %>% 
    filter(treatment == condition | treatment == "Control") %>% 
    summarize(sd = sd(Accuracy_conf)) %>% 
    pull(sd)
  
  power.t.test(n = n,
             delta = estimate,
             sd = sd,
             power = NULL) %>% 
  broom::tidy() %>% 
  mutate(condition = condition)
  
}

```

```{r}
results <- tibble() 

for(i in c("Combined", "CombinedQuiz", "TacticQuiz", "Tactics")){
  for(n in seq(1:1500)){
    
    results <- results %>% 
      rbind(calculate_power(condition = i,
                            n = n))
    
  }
}
```

```{r}
results %>% 
  filter(n <= 750) %>% 
  ggplot(aes(n, power, color = condition)) +
  geom_hline(aes(yintercept = .8), linetype = "dashed") +
  geom_point() +
  scale_x_continuous(breaks = seq(0, 800, 100)) +
  scale_y_continuous(labels = scales::percent_format(), breaks = seq(0, 1, .1)) +
  labs(title = "Power Calculations",
       subtitle = "Each condition compared to control group",
       caption = "Source: First Draft Pilot Survey",
       x = "n per group",
       y = "power") +
  theme_bw()
```


```{r}
results_effects <- tibble() 

for(i in c("Combined", "CombinedQuiz", "TacticQuiz", "Tactics")){
  for(n in seq(1:750)){
    for(e in seq(0, 2, .1)){
      
      results_effects <- results_effects %>% 
        rbind(calculate_power(condition = i,
                              n = n,
                              estimate = e))
    }
  }
}
```

```{r}
results_effects %>% 
  mutate(per_change = delta / 2.542623) %>% 
  filter(n < 700) %>% 
  ggplot(aes(n, per_change)) +
  geom_tile(aes(fill = power)) +
  scale_fill_gradient2(low = "red",
                       mid = "yellow",
                       high = "chartreuse4",
                       midpoint = .8) +
  scale_x_continuous(breaks = seq(0, 800, 50)) +
  scale_y_continuous(breaks = seq(0, 1, .1), labels = scales::percent_format()) +
  labs(title = "Power Calculations",
       subtitle = "Green = >80% power; Yellow = 80% power; Red = <80% power",
       caption = "Source: SDs calculate from First Draft Pilot Survey",
       x = "n per group",
       y = "% improvement") +
  theme_minimal()
```



```{r}
results_effects %>% 
  mutate(per_change = delta / 2.542623) %>% 
  filter(n < 700) %>% 
  ggplot(aes(n, per_change)) +
  geom_tile(aes(fill = power)) +
  scale_fill_gradient2(low = "red",
                       mid = "yellow",
                       high = "chartreuse4",
                       midpoint = .8) +
  scale_x_continuous(breaks = seq(0, 800, 50)) +
  scale_y_continuous(breaks = seq(0, 1, .1), labels = scales::percent_format()) +
  labs(title = "Power Calculations",
       subtitle = "Green = >80% power; Yellow = 80% power; Red = <80% power",
       caption = "Source: SDs calculate from First Draft Pilot Survey",
       x = "n per group",
       y = "% improvement") +
  theme_minimal() +
  annotate("rect", xmin = 1950, xmax = 1980, ymin = -1, ymax = 1,
           alpha = .1,fill = "blue")
```


# Main Question

Posts 1, 4, 5, 7, 8, and 9 contain misinformation: 

```{r}
pre_true <- sample(c("A2", "A3", "A6", "A10"), 2)
pre_false <- sample(c("A1", "A4", "A5", "A7", "A8", "A9"), 3)


data %>% 
  select(X, treatment, A1:A10) %>% 
  filter(treatment == "Control") %>% 
  pivot_longer(cols = c("A1", "A2", "A3", "A4", "A5", "A6", "A7", "A8", "A9", "A10")) %>% 
  mutate(period = if_else(name %in% pre_true | name %in% pre_false, "pre", "post")) %>% 
  group_by(X, period) %>% 
  summarize(mean_value = mean(value)) %>% 
  pivot_wider(names_from = period, values_from = mean_value) %>% 
  mutate(delta = post - pre) %>% 
  ungroup() %>% 
  summarize(sd = sd(delta))
```

```{r}
pre_true <- sample(c("A2_conf", "A3_conf", "A6_conf", "A10_conf"), 2)
pre_false <- sample(c("A1_conf", "A4_conf", "A5_conf", "A7_conf", "A8_conf", "A9_conf"), 3)


data %>% 
  select(X, treatment, A1:A10, X01C_1:X10C_1) %>% 
  filter(treatment == "Control") %>% 
  mutate(A1_conf = A1 * X01C_1,
         A2_conf = A2 * X02C_1,
         A3_conf = A3 * X03C_1,
         A4_conf = A4 * X04C_1,
         A5_conf = A5 * X05C_1,
         A6_conf = A6 * X06C_1,
         A7_conf = A7 * X07C_1,
         A8_conf = A8 * X08C_1,
         A9_conf = A9 * X09C_1,
         A10_conf = A10 * X10C_1) %>% 
  pivot_longer(cols = c("A1_conf", "A2_conf", "A3_conf", "A4_conf", "A5_conf", "A6_conf", "A7_conf", "A8_conf", "A9_conf", "A10_conf")) %>% 
  mutate(period = if_else(name %in% pre_true | name %in% pre_false, "pre", "post"),
         value = scales::rescale(value, c(1, 6))) %>% 
  group_by(X, period) %>% 
  summarize(mean_value = mean(value)) %>% 
  pivot_wider(names_from = period, values_from = mean_value) %>% 
  mutate(delta = post - pre) %>% 
  ungroup() %>% 
  summarize(mean = mean(delta),
            sd = sd(delta))
```

```{r}
data_match <- data %>% 
  mutate(treatment_binary = if_else(treatment == "Control", 1, 0)) %>% 
  select(X, treatment_binary, age, gender, hhi, ethnicity, hispanic, education, region, Ideology_numeric, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10)
```

```{r}
m.out1 <- MatchIt::matchit(treatment_binary ~ age + gender + hhi + ethnicity + hispanic + education + region + Ideology_numeric, 
                  data = data_match,
                  method = "nearest",
                  distance = "glm")
```

```{r}
data %>% 
  mutate(treatment_binary = if_else(treatment == "Control", 1, 0)) %>% 
  cbind(match_number = m.out1$subclass) %>% 
  mutate(A1_conf = A1 * X01C_1,
         A2_conf = A2 * X02C_1,
         A3_conf = A3 * X03C_1,
         A4_conf = A4 * X04C_1,
         A5_conf = A5 * X05C_1,
         A6_conf = A6 * X06C_1,
         A7_conf = A7 * X07C_1,
         A8_conf = A8 * X08C_1,
         A9_conf = A9 * X09C_1,
         A10_conf = A10 * X10C_1) %>% 
  pivot_longer(cols = c("A1_conf", "A2_conf", "A3_conf", "A4_conf", "A5_conf", "A6_conf", "A7_conf", "A8_conf", "A9_conf", "A10_conf")) %>% 
  mutate(value = scales::rescale(value, c(1, 6))) %>% 
  group_by(X, treatment_binary, match_number) %>% 
  summarize(mean_value = mean(value)) %>% 
  ungroup() %>% 
  filter(!is.na(match_number)) %>% 
  select(-X) %>% 
  pivot_wider(names_from = treatment_binary, values_from = mean_value) %>% 
  mutate(delta = `1` - `0`) %>% 
  summarize(mean = mean(delta),
            sd = sd(delta))
```

```{r}
data %>% 
  filter(treatment == "Control") %>% 
  mutate(A1_conf = A1 * X01C_1,
         A2_conf = A2 * X02C_1,
         A3_conf = A3 * X03C_1,
         A4_conf = A4 * X04C_1,
         A5_conf = A5 * X05C_1,
         A6_conf = A6 * X06C_1,
         A7_conf = A7 * X07C_1,
         A8_conf = A8 * X08C_1,
         A9_conf = A9 * X09C_1,
         A10_conf = A10 * X10C_1) %>% 
  pivot_longer(cols = c("A1_conf", "A2_conf", "A3_conf", "A4_conf", "A5_conf", "A6_conf", "A7_conf", "A8_conf", "A9_conf", "A10_conf")) %>% 
  mutate(value = scales::rescale(value, c(1, 6))) %>% 
  summarize(mean = mean(value))
```



```{r}

calculate_power <- function(n1,
                              n2,
                              split,
                              treatment_mean,
                              treatment_sd,
                              control_mean,
                              control_sd){
  
  treatment_group <- rnorm(n1, treatment_mean, treatment_sd)
  control_group <- rnorm(n2, control_mean, control_sd)
  
  t.test(treatment_group, control_group) %>% broom::tidy()
}

```

```{r}
power_sims <- function(n1,
                       n2,
                       split,
                       treatment_mean,
                       treatment_sd,
                       control_mean,
                       control_sd,
                       p_adjustments){
  power_results <- tibble()
for(i in 1:100){
  power_results <- power_results %>% 
    rbind(calculate_power(n1, n2, split, treatment_mean, treatment_sd, control_mean, control_sd) %>% mutate(i = i))
}

power_results %>% 
  mutate(row_n = row_number()) %>% 
  group_by(row_n) %>% 
  mutate(p_value_adjusted = p.adjust(p.value, method = "BH", n = p_adjustments)) %>% 
  mutate(significant = if_else(p_value_adjusted < .05, 1, 0)) %>% 
  ungroup() %>% 
  summarize(power = sum(significant) / 100) %>% 
  mutate(N = N, split = split, treatment_mean = treatment_mean, treatment_sd = treatment_sd, control_mean = control_mean, control_sd = control_sd, p_adjustments)
}

```

```{r}
n1 <- 
n2 <- 
split <- 2/3
treatment_mean <- NULL
treatment_sd <- 0.9862187
control_mean <- 0
control_sd = 0.7791459
p_adjustments <- 1

power_calcs_1 <- tibble()

for(N in seq(100, 1500, 25)){
  print(N)
  for(treatment_mean in seq(0, .5, .02)){
    power_calcs_1 <- power_calcs_1 %>% 
      rbind(power_sims(N,
                       split,
                       treatment_mean,
                       treatment_sd,
                       control_mean,
                       control_sd,
                       p_adjustments))
  }
}
```

```{r}
power_calcs_1 %>%
  mutate(per_improvement = treatment_mean / 4.135656) %>% 
  ggplot(aes(N, per_improvement)) +
  geom_tile(aes(fill = power)) +
  scale_fill_gradient2(low = "red",
                       mid = "yellow",
                       high = "chartreuse4",
                       midpoint = .8) +
  scale_x_continuous(breaks = seq(0, 1500, 100)) +
  scale_y_continuous(breaks = seq(0, 1, .01), labels = scales::percent_format()) +
  labs(title = "Power Calculations",
       subtitle = "Green = >80% power; Yellow = 80% power; Red = <80% power",
       caption = "Even test-control split",
       x = "Total N",
       y = "% improvement") +
  theme_minimal()
```


```{r}
N <- NULL
split <- NULL
treatment_mean <- .21
treatment_sd <- 0.9862187
control_mean <- 0
control_sd = 0.7791459
p_adjustments <- 1

power_calcs_2 <- tibble()

for(N in seq(100, 1500, 25)){
  print(N)
  for(split in seq(.5, .9, .05)){
    power_calcs_2 <- power_calcs_2 %>% 
      rbind(power_sims(N,
                       split,
                       treatment_mean,
                       treatment_sd,
                       control_mean,
                       control_sd,
                       p_adjustments))
  }
}
```

```{r}
power_calcs_2 %>%
  mutate(per_improvement = treatment_mean / 4.135656) %>% 
  ggplot(aes(N, split)) +
  geom_tile(aes(fill = power)) +
  scale_fill_gradient2(low = "red",
                       mid = "yellow",
                       high = "chartreuse4",
                       midpoint = .8) +
  scale_x_continuous(breaks = seq(0, 1500, 100)) +
  scale_y_continuous(breaks = seq(0, 1, .1), labels = scales::percent_format()) +
  labs(title = "Power Calculations",
       subtitle = "Green = >80% power; Yellow = 80% power; Red = <80% power",
       caption = "~5% Treatment Effect",
       x = "Total N",
       y = "% split in test group") +
  theme_minimal()
```


```{r}
N <- NULL
split <- 2/3
treatment_mean <- NULL
treatment_sd <- 0.9862187
control_mean <- 0
control_sd = 0.7791459
p_adjustments <- 9

power_calcs_3 <- tibble()

for(N in seq(100, 1500, 25)){
  print(N)
  for(treatment_mean in seq(0, .5, .02)){
    power_calcs_3 <- power_calcs_3 %>% 
      rbind(power_sims(N,
                       split,
                       treatment_mean,
                       treatment_sd,
                       control_mean,
                       control_sd,
                       p_adjustments))
  }
}
```

```{r}
power_calcs_3 %>%
  mutate(per_improvement = treatment_mean / 4.135656) %>% 
  ggplot(aes(N, per_improvement)) +
  geom_tile(aes(fill = power)) +
  scale_fill_gradient2(low = "red",
                       mid = "yellow",
                       high = "chartreuse4",
                       midpoint = .8) +
  scale_x_continuous(breaks = seq(0, 1500, 100)) +
  scale_y_continuous(breaks = seq(0, 1, .01), labels = scales::percent_format()) +
  labs(title = "Power Calculations w/ 9 corrections",
       subtitle = "Green = >80% power; Yellow = 80% power; Red = <80% power",
       caption = "66% in Treatment Group",
       x = "Total N",
       y = "% improvement") +
  theme_minimal()
```

```{r}
N <- NULL
split <- NULL
treatment_mean <- .21
treatment_sd <- 0.9862187
control_mean <- 0
control_sd = 0.7791459
p_adjustments <- 9

power_calcs_4 <- tibble()

for(N in seq(100, 1500, 25)){
  print(N)
  for(split in seq(.5, .9, .01)){
    power_calcs_4 <- power_calcs_4 %>% 
      rbind(power_sims(N,
                       split,
                       treatment_mean,
                       treatment_sd,
                       control_mean,
                       control_sd,
                       p_adjustments))
  }
}
```

```{r}
power_calcs_4 %>%
  ggplot(aes(N, split)) +
  geom_tile(aes(fill = power)) +
  scale_fill_gradient2(low = "red",
                       mid = "yellow",
                       high = "chartreuse4",
                       midpoint = .8) +
  scale_x_continuous(breaks = seq(0, 1500, 100)) +
  scale_y_continuous(breaks = seq(0, 1, .1), labels = scales::percent_format()) +
  labs(title = "Power Calculations w/ 9 corrections",
       subtitle = "Green = >80% power; Yellow = 80% power; Red = <80% power",
       caption = "~5% Treatment Effect",
       x = "Total N",
       y = "% split in test group") +
  theme_minimal()
```

```{r}
N <- 800
split <- NULL
treatment_mean <- NULL
treatment_sd <- 0.9862187
control_mean <- 0
control_sd = 0.7791459
p_adjustments <- 9

power_calcs_5 <- tibble()

for(treatment_mean in seq(0, .5, .02)){
  print(treatment_mean)
  for(split in seq(.5, .9, .01)){
    power_calcs_5 <- power_calcs_5 %>% 
      rbind(power_sims(N,
                       split,
                       treatment_mean,
                       treatment_sd,
                       control_mean,
                       control_sd,
                       p_adjustments))
  }
}
```

```{r}
N <- 800
split <- 2/3
treatment_mean <- NULL
treatment_sd <- 0.9862187
control_mean <- 0
control_sd = 0.7791459
p_adjustments <- NULL

power_calcs_6 <- tibble()

for(treatment_mean in seq(0, .5, .02)){
  print(treatment_mean)
  for(p_adjustments in 1:20){
    power_calcs_6 <- power_calcs_6 %>% 
      rbind(power_sims(N,
                       split,
                       treatment_mean,
                       treatment_sd,
                       control_mean,
                       control_sd,
                       p_adjustments))
  }
}
```

```{r}
power_calcs_5 %>%
  mutate(per_improvement = treatment_mean / 4.135656) %>% 
  ggplot(aes(per_improvement, p_adjustments)) +
  geom_tile(aes(fill = power)) +
  scale_fill_gradient2(low = "red",
                       mid = "yellow",
                       high = "chartreuse4",
                       midpoint = .8) +
  scale_x_continuous(breaks = seq(0, 1, .01), labels = scales::percent_format()) +
  scale_y_continuous(breaks = seq(0, 20, 2)) +
  labs(title = "Power Calculations",
       subtitle = "Green = >80% power; Yellow = 80% power; Red = <80% power",
       caption = "66% in Treatment Group, N = 800",
       x = "% improvement",
       y = "# of p-value adjustments") +
  theme_minimal()
```


# Binary

Posts 1, 4, 5, 7, 8, and 9 contain misinformation: 

```{r}
pre_true <- sample(c("A2", "A3", "A6", "A10"), 2)
pre_false <- sample(c("A1", "A4", "A5", "A7", "A8", "A9"), 3)

data %>% 
  select(X, treatment, A1:A10, X01C_1:X10C_1) %>% 
  filter(treatment == "Control") %>% 
  pivot_longer(cols = c("A1", "A2", "A3", "A4", "A5", "A6", "A7", "A8", "A9", "A10")) %>% 
  mutate(period = if_else(name %in% pre_true | name %in% pre_false, "pre", "post"),
         value = if_else(value == -1, 0, value)) %>% 
  group_by(X, period) %>% 
  summarize(mean_value = mean(value)) %>% 
  pivot_wider(names_from = period, values_from = mean_value) %>% 
  mutate(delta = post - pre) %>% 
  ungroup() %>% 
  summarize(mean = mean(delta),
            sd = sd(delta))
```

```{r}
data_match <- data %>% 
  mutate(treatment_binary = if_else(treatment == "Control", 1, 0)) %>% 
  select(X, treatment_binary, age, gender, hhi, ethnicity, hispanic, education, region, Ideology_numeric, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10)
```

```{r}
m.out1 <- MatchIt::matchit(treatment_binary ~ age + gender + hhi + ethnicity + hispanic + education + region + Ideology_numeric, 
                  data = data_match,
                  method = "nearest",
                  distance = "glm")
```

```{r}
data %>% 
  mutate(treatment_binary = if_else(treatment == "Control", 1, 0)) %>% 
  cbind(match_number = m.out1$subclass) %>% 
  pivot_longer(cols = c("A1", "A2", "A3", "A4", "A5", "A6", "A7", "A8", "A9", "A10")) %>% 
  mutate(value = if_else(value == -1, 0, value)) %>% 
  group_by(X, treatment_binary, match_number) %>% 
  summarize(mean_value = mean(value)) %>% 
  ungroup() %>% 
  filter(!is.na(match_number)) %>% 
  select(-X) %>% 
  pivot_wider(names_from = treatment_binary, values_from = mean_value) %>% 
  mutate(delta = `1` - `0`) %>% 
  summarize(mean = mean(delta),
            sd = sd(delta))
```


```{r}
N <- NULL
split <- .5
treatment_mean <- NULL
treatment_sd <- 0.3228544
control_mean <- 0
control_sd = 0.266571
p_adjustments <- 8

power_calcs_3_b <- tibble()

for(N in seq(100, 1500, 25)){
  print(N)
  for(treatment_mean in seq(0, .1, .01)){
    power_calcs_3_b <- power_calcs_3_b %>% 
      rbind(power_sims(N,
                       split,
                       treatment_mean,
                       treatment_sd,
                       control_mean,
                       control_sd,
                       p_adjustments))
  }
}
```

```{r}
power_calcs_3_b %>%
  mutate(per_improvement = treatment_mean / 0.5262295) %>% 
  ggplot(aes(N, per_improvement)) +
  geom_tile(aes(fill = power)) +
  scale_fill_gradient2(low = "red",
                       mid = "yellow",
                       high = "chartreuse4",
                       midpoint = .8) +
  scale_x_continuous(breaks = seq(0, 1500, 100)) +
  scale_y_continuous(breaks = seq(0, 1, .02), labels = scales::percent_format()) +
  labs(title = "Power Calculations w/ 8 corrections",
       subtitle = "Green = >80% power; Yellow = 80% power; Red = <80% power",
       caption = "Binary Questions | Even test-control split",
       x = "Total N",
       y = "% improvement") +
  theme_minimal()
```

```{r}
N <- NULL
split <- NULL
treatment_mean <- 0.026311475
treatment_sd <- 0.3228544
control_mean <- 0
control_sd = 0.266571
p_adjustments <- 9

power_calcs_4_b <- tibble()

for(N in seq(100, 1500, 25)){
  print(N)
  for(split in seq(.5, .9, .01)){
    power_calcs_4_b <- power_calcs_4_b %>% 
      rbind(power_sims(N,
                       split,
                       treatment_mean,
                       treatment_sd,
                       control_mean,
                       control_sd,
                       p_adjustments))
  }
}
```

```{r}
power_calcs_4_b %>%
  ggplot(aes(N, split)) +
  geom_tile(aes(fill = power)) +
  scale_fill_gradient2(low = "red",
                       mid = "yellow",
                       high = "chartreuse4",
                       midpoint = .8) +
  scale_x_continuous(breaks = seq(0, 1500, 100)) +
  scale_y_continuous(breaks = seq(0, 1, .1), labels = scales::percent_format()) +
  labs(title = "Power Calculations w/ 9 corrections",
       subtitle = "Green = >80% power; Yellow = 80% power; Red = <80% power",
       caption = "Binary Questions | ~5% Treatment Effect",
       x = "Total N",
       y = "% split in test group") +
  theme_minimal()
```










